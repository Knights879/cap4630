# -*- coding: utf-8 -*-
"""hw2p3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Do0WXctnn5eDjGvOVYfaGiXSbvTfODtP

## Loading the CIFAR10 data set
"""

# Commented out IPython magic to ensure Python compatibility.
import keras
from keras.datasets import cifar10
from keras.layers import Dense, Conv2D, BatchNormalization, Activation
from keras.layers import AveragePooling2D, Input, Flatten
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint, LearningRateScheduler
from keras.callbacks import ReduceLROnPlateau
from keras.preprocessing.image import ImageDataGenerator
from keras.regularizers import l2
from keras import backend as K
from keras import Model
from keras.utils.vis_utils import model_to_dot
import numpy as np
import os
from IPython.display import SVG
# %matplotlib inline
import matplotlib.pyplot as plt

# Load the CIFAR10 data
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

class_names = ['airplan', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

"""## Model parameters"""

# Training parameters
batch_size = 64
epochs = 20
data_augmentation = True
num_classes = 10

# Subtracting pixel mean improves accuracy
subtract_pixel_mean = True

# Model parameter
n = 3

# Model version
version = 1

# Computed depth from supplied model parameter n
if version == 1:
    depth = n * 6 + 2
elif version == 2:
    depth = n * 9 + 2

# Model name, depth and version
model_type = 'ResNet%dv%d' % (depth, version)

print(model_type)

"""## Preprocess the data"""

# Normalize data
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

# Convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

# Input dimensions
input_shape = x_train.shape[1:]

# Number of non-test examples
num_non_test = x_train.shape[0] 

# shuffle non-test data
shuffled_indices = np.random.permutation(num_non_test)
x_train = x_train[shuffled_indices]
y_train = y_train[shuffled_indices]

# Simple hold-out validation 
# Note that this could be also achieved by setting validation_split to 0.2

# Number of training examples
num_train = int(0.8 * num_non_test)

# Split into validation and train data
x_val = x_train[num_train:]
y_val = y_train[num_train:]

x_train = x_train[:num_train]
y_train = y_train[:num_train]

print('shapes')
print()
print('x_train:', x_train.shape)
print('y_train:', y_train.shape)
print('x_val  :', x_val.shape)
print('y_val  :', y_val.shape)
print('x_test :', x_test.shape)
print('y_test :', y_test.shape)

"""## Model 1 - Underfitting"""

# set up the layers
model1 = keras.Sequential([
    keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)),
    keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),
    keras.layers.MaxPooling2D((20, 20)),
    keras.layers.Flatten(),
    keras.layers.Dense(128, activation='relu', kernel_initializer='he_uniform'),
    keras.layers.Dense(10, activation='softmax')
])

# compile the model
model1.compile(optimizer='adam',
             loss='categorical_crossentropy',
             metrics=['accuracy'])

# train the model
epochs = 20
history = model1.fit(x_train, y_train,
                        batch_size=64,
                        epochs=epochs,
                        validation_data=(x_val, y_val))

# Evaluate accuracy
test_loss1, test_acc1 = model1.evaluate(x_test, y_test)

print('Test accuracy:', test_acc1)

# Training/test loss/accuracy during training
history_dict = history.history
loss_values = history_dict['loss']
test_loss_values = history_dict['val_loss']
epochs_range = range(1, epochs+1)

plt.plot(epochs_range, loss_values, 'bo', label='Training loss')
plt.plot(epochs_range, test_loss_values, 'ro', label='Test loss')
plt.title('Training and test loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

acc_values = history_dict['acc']
test_acc_values = history_dict['val_acc']

plt.plot(epochs_range, acc_values, 'bo', label='Training accuracy')
plt.plot(epochs_range, test_acc_values, 'ro', label='Test accuracy')
plt.title('Training and test accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""## Model 2 - Overfitting"""

# set up the layers
model2 = keras.Sequential([
    keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)),
    keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Flatten(),
    keras.layers.Dense(128, activation='relu', kernel_initializer='he_uniform'),
    keras.layers.Dense(10, activation='softmax')
])

# compile the model
model2.compile(optimizer='adam',
             loss='categorical_crossentropy',
             metrics=['accuracy'])

# train the model
epochs = 20
history = model2.fit(x_train, y_train,
                        batch_size=64,
                        epochs=epochs,
                        validation_data=(x_val, y_val))

# Evaluate accuracy
test_loss2, test_acc2 = model2.evaluate(x_test, y_test)

print('Test accuracy:', test_acc2)

# Training/test loss/accuracy during training
history_dict = history.history
loss_values = history_dict['loss']
test_loss_values = history_dict['val_loss']
epochs_range = range(1, epochs+1)

plt.plot(epochs_range, loss_values, 'bo', label='Training loss')
plt.plot(epochs_range, test_loss_values, 'ro', label='Test loss')
plt.title('Training and test loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

acc_values = history_dict['acc']
test_acc_values = history_dict['val_acc']

plt.plot(epochs_range, acc_values, 'bo', label='Training accuracy')
plt.plot(epochs_range, test_acc_values, 'ro', label='Test accuracy')
plt.title('Training and test accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""## Model 3 - Pretty Good"""

# set up the layers
model3 = keras.Sequential([
    keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)),
    keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Dropout(0.2),
    keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),
    keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Dropout(0.2),
    keras.layers.Flatten(),
    keras.layers.Dense(128, activation='relu', kernel_initializer='he_uniform'),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation='softmax')
])

# compile the model
model3.compile(optimizer='adam',
             loss='categorical_crossentropy',
             metrics=['accuracy'])

# train the model
epochs = 10
history = model3.fit(x_train, y_train,
                        batch_size=64,
                        epochs=epochs,
                        validation_data=(x_val, y_val))

# Evaluate accuracy
test_loss3, test_acc3 = model3.evaluate(x_test, y_test)

print('Test accuracy:', test_acc3)

# Training/test loss/accuracy during training
history_dict = history.history
loss_values = history_dict['loss']
test_loss_values = history_dict['val_loss']
epochs_range = range(1, epochs+1)

plt.plot(epochs_range, loss_values, 'bo', label='Training loss')
plt.plot(epochs_range, test_loss_values, 'ro', label='Test loss')
plt.title('Training and test loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

acc_values = history_dict['acc']
test_acc_values = history_dict['val_acc']

plt.plot(epochs_range, acc_values, 'bo', label='Training accuracy')
plt.plot(epochs_range, test_acc_values, 'ro', label='Test accuracy')
plt.title('Training and test accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()