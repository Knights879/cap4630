# -*- coding: utf-8 -*-
"""hw2p1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xdMlfCwUBJOa0_Eu-h8-_sTLUCG8q7M3
"""

import keras
from keras import models
from keras import layers
from keras.datasets import mnist
from keras.layers import merge
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

(train_images_original, train_labels_original), (test_images_original, test_labels_original) = mnist.load_data()

train_images = train_images_original.reshape((60000, 28 * 28))
train_images = train_images.astype('float32') / 255

test_images = test_images_original.reshape((10000, 28 * 28))
test_images = test_images.astype('float32') / 255

train_labels = to_categorical(train_labels_original)
test_labels = to_categorical(test_labels_original)

train_images.shape

train_images_original.shape

vec = np.zeros([60000, 1])
vec.shape

i = 0
for images in train_images:
  # Handcrafted features
  f1 = train_images[i].mean()
  # Throw the handcrafted features into a vector and concat it with the flattened image vector
  vec[i] = np.array([f1])
  #train_images[i] = np.concatenate((train_images[i], vec))
  i=i+1

vec2 = np.zeros([10000, 1])
vec2.shape

i = 0
for images in test_images:
  # Handcrafted features
  f1 = test_images[i].mean()
  # Throw the handcrafted features into a vector and concat it with the flattened image vector
  vec2[i] = np.array([f1])
  #train_images[i] = np.concatenate((train_images[i], vec))
  i=i+1

np.concatenate((train_images, vec), axis=1).shape

network = models.Sequential()
network.add(layers.Dense(512, activation='relu', input_shape=(785,)))
network.add(layers.Dense(10, activation='softmax'))
network.summary()

network.compile(optimizer='rmsprop',
                loss='categorical_crossentropy',
                metrics=['accuracy'])

epochs = 10
history = network.fit(np.concatenate((train_images, vec), axis=1), 
                      train_labels, 
                      epochs=epochs, 
                      batch_size=128, 
                      validation_data=(np.concatenate((test_images, vec2), axis=1), test_labels))

history_dict = history.history
loss_values = history_dict['loss']
test_loss_values = history_dict['val_loss']
epochs_range = range(1, epochs + 1)

plt.plot(epochs_range, loss_values, 'bo', label='Training loss')
plt.plot(epochs_range, test_loss_values, 'ro', label='Test loss')
plt.title('Training and test loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

acc_values = history_dict['acc']
test_acc_values = history_dict['val_acc']

plt.plot(epochs_range, acc_values, 'bo', label='Training accuracy')
plt.plot(epochs_range, test_acc_values, 'ro', label='Test accuracy')
plt.title('Training and test accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

