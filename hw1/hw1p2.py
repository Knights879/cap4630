# -*- coding: utf-8 -*-
"""hw1p2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DgVEXbkWhnhPpez4pgt9bY0zMf5diLHf
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
from matplotlib import cm
from mpl_toolkits import mplot3d
# %matplotlib inline

np.random.seed(42)

xs = 2 * np.random.rand(100, 1)
xs2 = 2 * np.random.rand(100, 1)
ys = 4 + 3 * xs + np.random.rand(100, 1)

fig = plt.figure()
ax = plt.axes(projection='3d')
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('z')
ax.scatter(xs, xs2, ys, 'bo')

plt.show()

# split the data into training and test sets
# train set
train_xs = xs[:80]
train_xs2 = xs2[:80]
train_ys = ys[:80]
# test set
test_xs = xs[80:]
test_xs2 = xs2[80:]
test_ys = ys[80:]

# number of epochs
epochs = 10
# learning rate
lr = 0.01

# initial value for weight w and bias b
w = np.random.randn(1)
w2 = np.random.randn(1)
b = np.zeros(1)

for epoch in np.arange(epochs):
  for i in np.arange(80):
    y_pred = w * train_xs[i] + w2 * train_xs2[i] + b
    
    grad_w = (y_pred - train_ys[i]) * train_xs[i]
    grad_w2 = (y_pred - train_ys[i]) * train_xs2[i]
    grad_b = (y_pred - train_ys[i])
    
    w -= lr * grad_w
    w2 -= lr * grad_w2
    b -= lr * grad_b

test_loss = 0
for i in np.arange(20):
  test_loss += 0.5 * (w * test_xs[i] + w * test_xs2[i] + b - test_ys[i]) ** 2
test_loss /= 20

test_loss

pred_ys = w * test_xs + w2 * test_xs2 + b

# plt.plot(test_xs, test_ys, "b.")
# plt.plot(test_xs, pred_ys, "r.") # predicted values

ax2 = plt.axes(projection='3d')
ax2.set_xlabel('x')
ax2.set_ylabel('y')
ax2.set_zlabel('z')
ax2.scatter(test_xs, test_xs2, test_ys, 'bo')
ax2.scatter(test_xs, test_xs2, pred_ys, 'bo')

plt.show()

b

w

w2